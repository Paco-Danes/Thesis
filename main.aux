\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gesture_recognition}
\citation{cnn_face_id}
\citation{Recurrent_nn_reid}
\citation{Gait_cnn}
\citation{Transformers_reid}
\citation{RSSI_indoorloc}
\citation{RSSI_indoorloc_new}
\citation{RSSI_reliability_indoorloc}
\citation{CSI_RSSI}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{tissues_waves_interaction}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Image-based person ID within a group of 5 people.\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:visionID}{{1.1}{2}{Image-based person ID within a group of 5 people.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The Wireless Channel}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Electromagnetic Waves}{3}{section.2.1}\protected@file@percent }
\newlabel{fig:wave}{{2.1a}{4}{Representation of a wave’s properties\relax }{figure.caption.3}{}}
\newlabel{sub@fig:wave}{{a}{4}{Representation of a wave’s properties\relax }{figure.caption.3}{}}
\newlabel{fig:phase}{{2.1b}{4}{Illustration of a phase shift.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:phase}{{b}{4}{Illustration of a phase shift.\relax }{figure.caption.3}{}}
\citation{OFDM_basics}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}I/Q sampling}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:IQ}{{2.2}{5}{I/Q sampling}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Decomposition of a wave in I/Q components.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:IQ_sample}{{2.2}{5}{Decomposition of a wave in I/Q components.\relax }{figure.caption.4}{}}
\citation{EdgeWiFiSensing2022}
\citation{modulations}
\citation{QAM_explained}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Orthogonal Frequency-Division Multiplexing}{6}{section.2.3}\protected@file@percent }
\newlabel{fig:sinc_func}{{2.3a}{6}{The $sinc$ function for a single subcarrier signal.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:sinc_func}{{a}{6}{The $sinc$ function for a single subcarrier signal.\relax }{figure.caption.5}{}}
\newlabel{fig:multiple_sinc}{{2.3b}{6}{Orthogonal $sinc$ functions for multiple subcarriers.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:multiple_sinc}{{b}{6}{Orthogonal $sinc$ functions for multiple subcarriers.\relax }{figure.caption.5}{}}
\newlabel{fig:summation}{{2.3c}{6}{Signals summation.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:summation}{{c}{6}{Signals summation.\relax }{figure.caption.5}{}}
\citation{ESP32_tool}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Channel State Information}{7}{section.2.4}\protected@file@percent }
\citation{hampel_ID}
\citation{Gait_hampel_pca}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Signal Preprocessing}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:preprocess}{{3}{8}{Signal Preprocessing}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Amplitude Sanitization}{8}{section.3.1}\protected@file@percent }
\newlabel{sec:amp_san}{{3.1}{8}{Amplitude Sanitization}{section.3.1}{}}
\citation{DWT_review}
\citation{phase_noise}
\citation{Phase_cal1}
\citation{Phase_cal3}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Raw and filtered amplitudes of a single subcarrier over 10 seconds of acquisitions.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Hampel1}{{3.1}{9}{Raw and filtered amplitudes of a single subcarrier over 10 seconds of acquisitions.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Phase Sanitization}{9}{section.3.2}\protected@file@percent }
\newlabel{sec:cal}{{3.2}{9}{Phase Sanitization}{section.3.2}{}}
\citation{Phase_cal2}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Raw and sanitized phases of subcarrier $9^{th}$ of 500 consecutive packets.\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:phase_cal}{{3.2}{10}{Raw and sanitized phases of subcarrier $9^{th}$ of 500 consecutive packets.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Data exploration and structuring}{11}{section.3.3}\protected@file@percent }
\newlabel{Sec:structuring}{{3.3}{11}{Data exploration and structuring}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Heatmaps of the amplitudes of an empty room vs the same room with 2 different people standing in the LOS path between the antennas.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:heatmap}{{3.3}{11}{Heatmaps of the amplitudes of an empty room vs the same room with 2 different people standing in the LOS path between the antennas.\relax }{figure.caption.8}{}}
\citation{WiWho_gait_decisionTree}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Proposed method}{13}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset acquisition}{13}{section.4.1}\protected@file@percent }
\citation{LeCun_CNN}
\citation{time_conv}
\citation{EEG_conv}
\citation{cnn_segment}
\citation{class_cnn}
\citation{Goodfellow_DL}
\newlabel{fig:3D_room}{{\caption@xref {fig:3D_room}{ on input line 365}}{14}{Dataset acquisition}{figure.caption.9}{}}
\newlabel{sub@fig:3D_room}{{}{14}{Dataset acquisition}{figure.caption.9}{}}
\newlabel{fig:2D_room}{{\caption@xref {fig:2D_room}{ on input line 370}}{14}{Dataset acquisition}{figure.caption.9}{}}
\newlabel{sub@fig:2D_room}{{}{14}{Dataset acquisition}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The room environment in which the data were taken (rendering).\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:room}{{4.1}{14}{The room environment in which the data were taken (rendering).\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Convolutional Neural Network}{14}{section.4.2}\protected@file@percent }
\newlabel{sec:CNN}{{4.2}{14}{Convolutional Neural Network}{section.4.2}{}}
\citation{rec_field}
\citation{rec_field}
\citation{pooling}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The receptive fields of the units in the deeper layers of a convolutional network is larger than the one of the units in the shallow layers. This effect increases with architectural features like pooling or strided convolutions. This means that even though direct connections are very sparse, units in the deeper layers \cite  {rec_field} can be indirectly connected to most of the input tensor.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:rec_field}{{4.2}{15}{The receptive fields of the units in the deeper layers of a convolutional network is larger than the one of the units in the shallow layers. This effect increases with architectural features like pooling or strided convolutions. This means that even though direct connections are very sparse, units in the deeper layers \cite {rec_field} can be indirectly connected to most of the input tensor.\relax }{figure.caption.10}{}}
\citation{padding_Conv}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Pooling and Padding}{16}{subsection.4.2.1}\protected@file@percent }
\citation{overfitting}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A Max Pooling operation with a kernel of size 3 and stride between pools of 2. This reduces the representation size in a half, decreasing computational requirements while preserving the most important information. Note that the last pooling region on the right is zero padded, which can be just intended as a pooling kernel of smaller size, and must be included in order to not ignore border units.\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:pooling}{{4.3}{17}{A Max Pooling operation with a kernel of size 3 and stride between pools of 2. This reduces the representation size in a half, decreasing computational requirements while preserving the most important information. Note that the last pooling region on the right is zero padded, which can be just intended as a pooling kernel of smaller size, and must be included in order to not ignore border units.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Dropout and Weight Decay}{17}{subsection.4.2.2}\protected@file@percent }
\newlabel{sec:dropout}{{4.2.2}{17}{Dropout and Weight Decay}{subsection.4.2.2}{}}
\citation{dropoput}
\citation{dropout_variants}
\citation{regular_comparison}
\citation{regular_combining}
\citation{survey_regular}
\citation{cnn_csi_activity}
\citation{cnn_csi_localization}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces A multilayer perceptron with dropout incorporated. Some units stochastically deactivate with probability $p$, creating a subnetwork that will be affected by backpropagation. At each update step, the units to be deactivated are resampled, obtaining new subnetworks time after time. This ensures that the model will not rely on just some "strong" neurons, but on a more distributed inference.\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:dropout}{{4.4}{19}{A multilayer perceptron with dropout incorporated. Some units stochastically deactivate with probability $p$, creating a subnetwork that will be affected by backpropagation. At each update step, the units to be deactivated are resampled, obtaining new subnetworks time after time. This ensures that the model will not rely on just some "strong" neurons, but on a more distributed inference.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Proposed Architecture}{20}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The Convolutional Neural Network architecture designed for the CSI-based person Identification task. The same model was employed for testing the performance with 2,3,4,5 and 6 classes by only adjusting the last fully connected output layer, while the rest of the architecture was always kept intact. The shapes, written above each feature map, refer to the tensors before applying any padding operation.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:model}{{4.5}{20}{The Convolutional Neural Network architecture designed for the CSI-based person Identification task. The same model was employed for testing the performance with 2,3,4,5 and 6 classes by only adjusting the last fully connected output layer, while the rest of the architecture was always kept intact. The shapes, written above each feature map, refer to the tensors before applying any padding operation.\relax }{figure.caption.13}{}}
\citation{spatial_dropout}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{22}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Working Pipeline}{22}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The complete workflow for person ID through Wi-Fi signals.\relax }}{22}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pipeline}{{5.1}{22}{The complete workflow for person ID through Wi-Fi signals.\relax }{figure.caption.14}{}}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Training}{23}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Validation results}{24}{section.5.3}\protected@file@percent }
\bibcite{gesture_recognition}{1}
\bibcite{cnn_face_id}{2}
\bibcite{Recurrent_nn_reid}{3}
\bibcite{Gait_cnn}{4}
\bibcite{Transformers_reid}{5}
\bibcite{RSSI_indoorloc}{6}
\bibcite{RSSI_indoorloc_new}{7}
\bibcite{RSSI_reliability_indoorloc}{8}
\bibcite{CSI_RSSI}{9}
\bibcite{tissues_waves_interaction}{10}
\bibcite{OFDM_basics}{11}
\bibcite{EdgeWiFiSensing2022}{12}
\bibcite{modulations}{13}
\bibcite{QAM_explained}{14}
\bibcite{ESP32_tool}{15}
\bibcite{hampel_ID}{16}
\bibcite{Gait_hampel_pca}{17}
\bibcite{DWT_review}{18}
\bibcite{phase_noise}{19}
\bibcite{Phase_cal1}{20}
\bibcite{Phase_cal3}{21}
\bibcite{Phase_cal2}{22}
\bibcite{WiWho_gait_decisionTree}{23}
\bibcite{LeCun_CNN}{24}
\bibcite{time_conv}{25}
\bibcite{EEG_conv}{26}
\bibcite{cnn_segment}{27}
\bibcite{class_cnn}{28}
\bibcite{Goodfellow_DL}{29}
\bibcite{rec_field}{30}
\bibcite{pooling}{31}
\bibcite{padding_Conv}{32}
\bibcite{overfitting}{33}
\bibcite{dropoput}{34}
\bibcite{dropout_variants}{35}
\bibcite{regular_comparison}{36}
\bibcite{regular_combining}{37}
\bibcite{survey_regular}{38}
\bibcite{cnn_csi_activity}{39}
\bibcite{cnn_csi_localization}{40}
\bibcite{spatial_dropout}{41}
\bibstyle{ieeetr}
\gdef \@abspage@last{32}
