{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import my_utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pywt\n",
    "import pywt.data\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet = 'haar'\n",
    "level = 5\n",
    "threshold = 0.7\n",
    "# Perform the DWT\n",
    "\n",
    "def soft_threshold(value, threshold):\n",
    "    return np.where(value > threshold, value - threshold, np.where(value < -threshold, value + threshold, 0.0))\n",
    "\n",
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 9):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\\\fra{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\\\fra{i}_calibratedPha.csv')\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorFra = np.array(train_sets)\n",
    "tensorTestFra = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 8):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    for i in range(1, 6):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "\n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorLaura = np.array(train_sets)\n",
    "tensorTestLaura = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 875, 200, 104) (5, 213, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 7):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorGio = np.array(train_sets)\n",
    "tensorTestGio = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 875, 200, 104) (5, 213, 200, 104)\n",
      "(5, 723, 200, 104) (5, 179, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)\n",
    "print(tensorGio.shape, tensorTestGio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    #tensor1 = torch.from_numpy(tensorEmpty)\n",
    "    tensor2 = torch.from_numpy(tensorGio[i])\n",
    "    tensor3 = torch.from_numpy(tensorLaura[i])\n",
    "    tensor4 = torch.from_numpy(tensorFra[i])\n",
    "    #tensor5 = torch.from_numpy(tensorEmpty)\n",
    "    # Assuming you have your data tensors tensor1 and tensor2 of shape (1135, 200, 104)\n",
    "    # Add a channel dimension to the input data\n",
    "    #tensor1 = tensor1.unsqueeze(1)  # Adds a channel dimension (1) at position 1\n",
    "    tensor2 = tensor2.unsqueeze(1)\n",
    "    tensor3 = tensor3.unsqueeze(1)\n",
    "    tensor4 = tensor4.unsqueeze(1)\n",
    "    #tensor5 = tensor5.unsqueeze(1)\n",
    "    # Assuming you have 455 images of class 1 and 680 images of class 2\n",
    "    #labels1 = torch.zeros(1103, dtype=torch.long)  # Class 1 labels as 0\n",
    "    labels2 = torch.zeros(723, dtype=torch.long)   # Class 2 labels as 1\n",
    "    labels3 = torch.ones(875, dtype=torch.long) # Class 3 labels as 2\n",
    "    labels4 = torch.ones(755, dtype=torch.long) + 1\n",
    "    #labels5 = torch.ones(895, dtype=torch.long) + 2\n",
    "    # Combine Data and Labels\n",
    "    data = torch.cat((tensor2, tensor3, tensor4), dim=0)\n",
    "    labels = torch.cat((labels2, labels3, labels4), dim=0)\n",
    "\n",
    "    #dataset = TensorDataset(data, labels) #dataset = TensorDataset(data, labels)\n",
    "    ### dataset = torch.load('..\\Data\\DatasetTensor\\empty_laura.pt')\n",
    "    train_dataset = TensorDataset(data,labels)\n",
    "\n",
    "    tt2 = torch.from_numpy(tensorTestGio[i])\n",
    "    tt3 = torch.from_numpy(tensorTestLaura[i])\n",
    "    tt4 = torch.from_numpy(tensorTestFra[i])\n",
    "    #tt5 = torch.from_numpy(tensorTestEmpty)\n",
    "    tt2 = tt2.unsqueeze(1)\n",
    "    tt3 = tt3.unsqueeze(1)\n",
    "    tt4 = tt4.unsqueeze(1)\n",
    "    #tt5 = tt5.unsqueeze(1)\n",
    "    ll2 = torch.zeros(179, dtype=torch.long)\n",
    "    ll3 = torch.ones(213, dtype=torch.long)\n",
    "    ll4 = torch.ones(184, dtype=torch.long) + 1\n",
    "    #ll5 = torch.ones(208, dtype=torch.long) + 2\n",
    "    dataT = torch.cat((tt2, tt3, tt4), dim=0)\n",
    "    labelsT = torch.cat((ll2, ll3, ll4), dim=0)\n",
    "    test_dataset = TensorDataset(dataT,labelsT)\n",
    "    '''\n",
    "    tt1 = torch.from_numpy(empty_test).unsqueeze(1)\n",
    "    tt2 = torch.from_numpy(laura_test).unsqueeze(1)\n",
    "    ll1 = torch.zeros(123, dtype=torch.long)\n",
    "    ll2 = torch.ones(125, dtype=torch.long)\n",
    "    dd = torch.cat((tt1, tt2), dim=0)\n",
    "    ll = torch.cat((ll1, ll2), dim=0)\n",
    "    test_dataset = TensorDataset(dd,ll)\n",
    "    '''\n",
    "    # Define the sizes for your train and test sets\n",
    "    #train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "    #test_size = len(dataset) - train_size  # 20% for testing\n",
    "    # Split the dataset into train and test\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    # Create DataLoaders\n",
    "    batch_size = 64  # Adjust as neededs\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    torch.save(train_dataset,f'models\\\\train_DWT_GioLaFra_{i}.pt')\n",
    "    torch.save(test_dataset,f'models\\\\test_DWT_GioLaFra_{i}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-fold cv LaFra fraGio   GioLA                      Ternary-class\n",
    "\n",
    "86.7           88.5            94.6                 85.2\n",
    "82.6           92.3            93.9                 89.2\n",
    "90.5           93.7            98.5                 96\n",
    "88.6           93.1            92.9                 92.5\n",
    "95.5           88.4            92.1                 89.2\n",
    "               Medie\n",
    "88.78           91.2           94.4s                90.4\n",
    "            media tot: 91.5\n",
    "\n",
    "            Train 100 epochs 0.001, 20 epochs 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with DWT\n",
    "91.2            94.2            94.4                95.5\n",
    "90.2            95.6            96.7\n",
    "92.7            93.9            98.5\n",
    "91.9            96.1            98.2\n",
    "88.4            93.9            97.2\n",
    "\n",
    "media = 90.9    94.74           97\n",
    "media delle medie = 94.21\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
