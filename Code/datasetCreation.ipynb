{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import my_utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pywt\n",
    "import pywt.data\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\\\andrea6_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\\\andrea6_calibratedPha.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\devin\\Desktop\\Università\\Tesi\\Thesis\\Code\\datasetCreation.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devin/Desktop/Universit%C3%A0/Tesi/Thesis/Code/datasetCreation.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m7\u001b[39m):  \u001b[39m# For each file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devin/Desktop/Universit%C3%A0/Tesi/Thesis/Code/datasetCreation.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     data_amp_original, data_pha \u001b[39m=\u001b[39m my_utils\u001b[39m.\u001b[39mextract_amp_phase(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRawData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mkaty\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, names\u001b[39m=\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDataClean\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mkaty\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_amp.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDataClean\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mkaty\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_phase.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/devin/Desktop/Universit%C3%A0/Tesi/Thesis/Code/datasetCreation.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     data_amp_original \u001b[39m=\u001b[39m my_utils\u001b[39m.\u001b[39;49mhampel_filtering(data_amp_original, window_size \u001b[39m=\u001b[39;49m \u001b[39m51\u001b[39;49m, smoothing_factor \u001b[39m=\u001b[39;49m \u001b[39m0.8\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devin/Desktop/Universit%C3%A0/Tesi/Thesis/Code/datasetCreation.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     data_amp_original\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataPreprocessed\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mkaty\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_w51_hamp_sf08.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/devin/Desktop/Universit%C3%A0/Tesi/Thesis/Code/datasetCreation.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     data_amp \u001b[39m=\u001b[39m my_utils\u001b[39m.\u001b[39mDWT_denoising(data_amp_original)\n",
      "File \u001b[1;32mc:\\Users\\devin\\Desktop\\Università\\Tesi\\Thesis\\Code\\my_utils.py:94\u001b[0m, in \u001b[0;36mhampel_filtering\u001b[1;34m(df, window_size, thresh, smoothing_factor)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m# Calculate the median and MAD for the window\u001b[39;00m\n\u001b[0;32m     93\u001b[0m median \u001b[39m=\u001b[39m window\u001b[39m.\u001b[39mmedian()\n\u001b[1;32m---> 94\u001b[0m mad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmedian(np\u001b[39m.\u001b[39;49mabs(window \u001b[39m-\u001b[39;49m median))\n\u001b[0;32m     96\u001b[0m \u001b[39m# Define a threshold for outlier detection (e.g., 3 times MAD)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m threshold \u001b[39m=\u001b[39m thresh \u001b[39m*\u001b[39m mad\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\devin\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3793\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3711\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[0;32m   3712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmedian\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, overwrite_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   3713\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3714\u001b[0m \u001b[39m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[0;32m   3715\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3791\u001b[0m \n\u001b[0;32m   3792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3793\u001b[0m     r, k \u001b[39m=\u001b[39m _ureduce(a, func\u001b[39m=\u001b[39;49m_median, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout,\n\u001b[0;32m   3794\u001b[0m                     overwrite_input\u001b[39m=\u001b[39;49moverwrite_input)\n\u001b[0;32m   3795\u001b[0m     \u001b[39mif\u001b[39;00m keepdims:\n\u001b[0;32m   3796\u001b[0m         \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mreshape(k)\n",
      "File \u001b[1;32mc:\\Users\\devin\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3702\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3700\u001b[0m     keepdim \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m a\u001b[39m.\u001b[39mndim\n\u001b[1;32m-> 3702\u001b[0m r \u001b[39m=\u001b[39m func(a, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3703\u001b[0m \u001b[39mreturn\u001b[39;00m r, keepdim\n",
      "File \u001b[1;32mc:\\Users\\devin\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3828\u001b[0m, in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3826\u001b[0m         part \u001b[39m=\u001b[39m a\n\u001b[0;32m   3827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3828\u001b[0m     part \u001b[39m=\u001b[39m partition(a, kth, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3830\u001b[0m \u001b[39mif\u001b[39;00m part\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():\n\u001b[0;32m   3831\u001b[0m     \u001b[39m# make 0-D arrays work\u001b[39;00m\n\u001b[0;32m   3832\u001b[0m     \u001b[39mreturn\u001b[39;00m part\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\devin\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:758\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     a \u001b[39m=\u001b[39m asanyarray(a)\u001b[39m.\u001b[39mcopy(order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mK\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 758\u001b[0m a\u001b[39m.\u001b[39;49mpartition(kth, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m    759\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 7):  # For each file\n",
    "        data_amp_original, data_pha = \n",
    "        data_amp = my_utils.DWT_denoising(data_amp_original)\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorKaty = np.array(train_sets)\n",
    "tensorTestKaty = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 8):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    for i in range(1, 6):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "\n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorLaura = np.array(train_sets)\n",
    "tensorTestLaura = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 875, 200, 104) (5, 213, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 7):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_calibratedPha.csv')\n",
    "\n",
    "        coeffs = pywt.wavedec(data_amp_original, wavelet, level=level, axis=0)\n",
    "        denoised_coeffs = [soft_threshold(coeff, threshold) for coeff in coeffs]\n",
    "        data_amp = pywt.waverec(denoised_coeffs, wavelet, axis=0)\n",
    "        if (data_amp_original.shape[0] % 2) == 1:\n",
    "            data_amp = data_amp[:-1]\n",
    "\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorGio = np.array(train_sets)\n",
    "tensorTestGio = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 875, 200, 104) (5, 213, 200, 104)\n",
      "(5, 723, 200, 104) (5, 179, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)\n",
    "print(tensorGio.shape, tensorTestGio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    #tensor1 = torch.from_numpy(tensorEmpty)\n",
    "    tensor2 = torch.from_numpy(tensorGio[i])\n",
    "    tensor3 = torch.from_numpy(tensorLaura[i])\n",
    "    tensor4 = torch.from_numpy(tensorFra[i])\n",
    "    #tensor5 = torch.from_numpy(tensorEmpty)\n",
    "    # Assuming you have your data tensors tensor1 and tensor2 of shape (1135, 200, 104)\n",
    "    # Add a channel dimension to the input data\n",
    "    #tensor1 = tensor1.unsqueeze(1)  # Adds a channel dimension (1) at position 1\n",
    "    tensor2 = tensor2.unsqueeze(1)\n",
    "    tensor3 = tensor3.unsqueeze(1)\n",
    "    tensor4 = tensor4.unsqueeze(1)\n",
    "    #tensor5 = tensor5.unsqueeze(1)\n",
    "    # Assuming you have 455 images of class 1 and 680 images of class 2\n",
    "    #labels1 = torch.zeros(1103, dtype=torch.long)  # Class 1 labels as 0\n",
    "    labels2 = torch.zeros(723, dtype=torch.long)   # Class 2 labels as 1\n",
    "    labels3 = torch.ones(875, dtype=torch.long) # Class 3 labels as 2\n",
    "    labels4 = torch.ones(755, dtype=torch.long) + 1\n",
    "    #labels5 = torch.ones(895, dtype=torch.long) + 2\n",
    "    # Combine Data and Labels\n",
    "    data = torch.cat((tensor2, tensor3, tensor4), dim=0)\n",
    "    labels = torch.cat((labels2, labels3, labels4), dim=0)\n",
    "\n",
    "    #dataset = TensorDataset(data, labels) #dataset = TensorDataset(data, labels)\n",
    "    ### dataset = torch.load('..\\Data\\DatasetTensor\\empty_laura.pt')\n",
    "    train_dataset = TensorDataset(data,labels)\n",
    "\n",
    "    tt2 = torch.from_numpy(tensorTestGio[i])\n",
    "    tt3 = torch.from_numpy(tensorTestLaura[i])\n",
    "    tt4 = torch.from_numpy(tensorTestFra[i])\n",
    "    #tt5 = torch.from_numpy(tensorTestEmpty)\n",
    "    tt2 = tt2.unsqueeze(1)\n",
    "    tt3 = tt3.unsqueeze(1)\n",
    "    tt4 = tt4.unsqueeze(1)\n",
    "    #tt5 = tt5.unsqueeze(1)\n",
    "    ll2 = torch.zeros(179, dtype=torch.long)\n",
    "    ll3 = torch.ones(213, dtype=torch.long)\n",
    "    ll4 = torch.ones(184, dtype=torch.long) + 1\n",
    "    #ll5 = torch.ones(208, dtype=torch.long) + 2\n",
    "    dataT = torch.cat((tt2, tt3, tt4), dim=0)\n",
    "    labelsT = torch.cat((ll2, ll3, ll4), dim=0)\n",
    "    test_dataset = TensorDataset(dataT,labelsT)\n",
    "    '''\n",
    "    tt1 = torch.from_numpy(empty_test).unsqueeze(1)\n",
    "    tt2 = torch.from_numpy(laura_test).unsqueeze(1)\n",
    "    ll1 = torch.zeros(123, dtype=torch.long)\n",
    "    ll2 = torch.ones(125, dtype=torch.long)\n",
    "    dd = torch.cat((tt1, tt2), dim=0)\n",
    "    ll = torch.cat((ll1, ll2), dim=0)\n",
    "    test_dataset = TensorDataset(dd,ll)\n",
    "    '''\n",
    "    # Define the sizes for your train and test sets\n",
    "    #train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "    #test_size = len(dataset) - train_size  # 20% for testing\n",
    "    # Split the dataset into train and test\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    # Create DataLoaders\n",
    "    batch_size = 64  # Adjust as neededs\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    torch.save(train_dataset,f'models\\\\train_DWT_GioLaFra_{i}.pt')\n",
    "    torch.save(test_dataset,f'models\\\\test_DWT_GioLaFra_{i}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-fold cv LaFra fraGio   GioLA                      Ternary-class\n",
    "\n",
    "86.7           88.5            94.6                 85.2\n",
    "82.6           92.3            93.9                 89.2\n",
    "90.5           93.7            98.5                 96\n",
    "88.6           93.1            92.9                 92.5\n",
    "95.5           88.4            92.1                 89.2\n",
    "               Medie\n",
    "88.78           91.2           94.4s                90.4\n",
    "            media tot: 91.5\n",
    "\n",
    "            Train 100 epochs 0.001, 20 epochs 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with DWT\n",
    "91.2            94.2            94.4                95.5\n",
    "90.2            95.6            96.7\n",
    "92.7            93.9            98.5\n",
    "91.9            96.1            98.2\n",
    "88.4            93.9            97.2\n",
    "\n",
    "media = 90.9    94.74           97\n",
    "media delle medie = 94.21\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
