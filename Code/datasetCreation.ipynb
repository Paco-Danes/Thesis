{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import my_utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pywt\n",
    "import pywt.data\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_amp_original, data_pha = pd.read_csv('..\\Data\\DataClean\\\\andrea6_amp.csv'), pd.read_csv('..\\Data\\DataClean\\\\andrea6_phase.csv')\n",
    "data_amp = my_utils.hampel_filtering(data_amp_original, window_size=51, smoothing_factor=0.8)\n",
    "data_amp.to_csv('..\\Data\\DataPreprocessed\\\\andrea6_w51_hamp_sf08.csv')\n",
    "data_pha = data_pha.apply(my_utils.phase_sanitization_inRange, axis=1)\n",
    "data_pha.to_csv('..\\Data\\DataPreprocessed\\\\andrea6_calibratedPha.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 9):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\\\fra{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\\\fra{i}_calibratedPha.csv')\n",
    "        data_amp = my_utils.DWT_denoising(data_amp_original)\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorAndrea = np.array(train_sets)\n",
    "tensorTestAndrea = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorFra = np.array(train_sets)\n",
    "tensorTestFra = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 8):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\laura{i}_calibratedPha.csv')\n",
    "\n",
    "        data_amp = my_utils.DWT_denoising(data_amp_original)\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    for i in range(1, 6):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\lauraDay2_{i}_calibratedPha.csv')\n",
    "\n",
    "        data_amp = my_utils.DWT_denoising(data_amp_original)\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "\n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorLaura = np.array(train_sets)\n",
    "tensorTestLaura = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 875, 200, 104) (5, 213, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train and test sets\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# Define the sliding window size (20% of the data)\n",
    "window_size = 0.2\n",
    "\n",
    "# Loop through the files and create train-test sets\n",
    "for j in range(5):  # For each fold\n",
    "    # Initialize lists to store train and test sets for this fold\n",
    "    fold_train_sets = []\n",
    "    fold_test_sets = []\n",
    "    \n",
    "    for i in range(1, 7):  # For each file\n",
    "        data_amp_original, data_pha = pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_w51_hamp_sf08.csv'), pd.read_csv(f'..\\Data\\DataPreprocessed\\gio{i}_calibratedPha.csv')\n",
    "\n",
    "        data_amp = my_utils.DWT_denoising(data_amp_original)\n",
    "        data_amp = pd.DataFrame(scaler.fit_transform(data_amp))\n",
    "        dataset = pd.DataFrame(my_utils.make_alternating(data_amp, data_pha))\n",
    "        dataset = my_utils.create_tensor(dataset)\n",
    "        \n",
    "        # Calculate the number of data points for the test set and train set\n",
    "        total_points = len(dataset)\n",
    "        test_size = int(total_points * window_size)\n",
    "        train_size = total_points - test_size\n",
    "        \n",
    "        # Calculate the starting index for the test set\n",
    "        test_start = j * test_size\n",
    "        \n",
    "        # Calculate the ending index for the test set\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        # Extract test and train sets using slicing\n",
    "        test_set = dataset[test_start:test_end]\n",
    "        train_set = np.concatenate([dataset[:test_start], dataset[test_end:]])\n",
    "        \n",
    "        # Append train and test sets to the respective fold lists\n",
    "        fold_train_sets.append(train_set)\n",
    "        fold_test_sets.append(test_set)\n",
    "    \n",
    "    # Merge train sets and test sets for this fold\n",
    "    train_sets.append(np.concatenate(fold_train_sets))\n",
    "    test_sets.append(np.concatenate(fold_test_sets))\n",
    "\n",
    "# Now you have 5 train-test set couples for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorGio = np.array(train_sets)\n",
    "tensorTestGio = np.array(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 875, 200, 104) (5, 213, 200, 104)\n",
      "(5, 755, 200, 104) (5, 184, 200, 104)\n",
      "(5, 723, 200, 104) (5, 179, 200, 104)\n"
     ]
    }
   ],
   "source": [
    "#print(tensorGio.shape, tensorTestGio.shape)\n",
    "print(tensorLaura.shape, tensorTestLaura.shape)\n",
    "print(tensorFra.shape, tensorTestFra.shape)\n",
    "print(tensorAndrea.shape, tensorTestAndrea.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    #tensor1 = torch.from_numpy(tensorEmpty)\n",
    "    #tensor2 = torch.from_numpy(tensorGio[i])\n",
    "    tensor3 = torch.from_numpy(tensorLaura[i])\n",
    "    tensor4 = torch.from_numpy(tensorFra[i])\n",
    "    tensor5 = torch.from_numpy(tensorAndrea[i])\n",
    "    # Assuming you have your data tensors tensor1 and tensor2 of shape (1135, 200, 104)\n",
    "    # Add a channel dimension to the input data\n",
    "    #tensor1 = tensor1.unsqueeze(1)  # Adds a channel dimension (1) at position 1\n",
    "    #tensor2 = tensor2.unsqueeze(1)\n",
    "    tensor3 = tensor3.unsqueeze(1)\n",
    "    tensor4 = tensor4.unsqueeze(1)\n",
    "    tensor5 = tensor5.unsqueeze(1)\n",
    "    # Assuming you have 455 images of class 1 and 680 images of class 2\n",
    "    #labels1 = torch.zeros(1103, dtype=torch.long)  # Class 1 labels as 0\n",
    "    #labels2 = torch.zeros(723, dtype=torch.long)   # Class 2 labels as 1\n",
    "    labels3 = torch.zeros(875, dtype=torch.long) # Class 3 labels as 2\n",
    "    labels4 = torch.ones(755, dtype=torch.long) \n",
    "    labels5 = torch.ones(723, dtype=torch.long) + 1\n",
    "    # Combine Data and Labels\n",
    "    data = torch.cat((tensor3, tensor4, tensor5), dim=0)\n",
    "    labels = torch.cat((labels3, labels4, labels5), dim=0)\n",
    "\n",
    "    #dataset = TensorDataset(data, labels) #dataset = TensorDataset(data, labels)\n",
    "    ### dataset = torch.load('..\\Data\\DatasetTensor\\empty_laura.pt')\n",
    "    train_dataset = TensorDataset(data,labels)\n",
    "\n",
    "    #tt2 = torch.from_numpy(tensorTestGio[i])\n",
    "    tt3 = torch.from_numpy(tensorTestLaura[i])\n",
    "    tt4 = torch.from_numpy(tensorTestFra[i])\n",
    "    tt5 = torch.from_numpy(tensorTestAndrea[i])\n",
    "    #tt2 = tt2.unsqueeze(1)\n",
    "    tt3 = tt3.unsqueeze(1)\n",
    "    tt4 = tt4.unsqueeze(1)\n",
    "    tt5 = tt5.unsqueeze(1)\n",
    "    #ll2 = torch.zeros(179, dtype=torch.long)\n",
    "    ll3 = torch.zeros(213, dtype=torch.long)\n",
    "    ll4 = torch.ones(184, dtype=torch.long) \n",
    "    ll5 = torch.ones(179, dtype=torch.long) + 1\n",
    "    dataT = torch.cat((tt3, tt4, tt5), dim=0)\n",
    "    labelsT = torch.cat((ll3, ll4, ll5), dim=0)\n",
    "    test_dataset = TensorDataset(dataT,labelsT)\n",
    "    '''\n",
    "    tt1 = torch.from_numpy(empty_test).unsqueeze(1)\n",
    "    tt2 = torch.from_numpy(laura_test).unsqueeze(1)\n",
    "    ll1 = torch.zeros(123, dtype=torch.long)\n",
    "    ll2 = torch.ones(125, dtype=torch.long)\n",
    "    dd = torch.cat((tt1, tt2), dim=0)\n",
    "    ll = torch.cat((ll1, ll2), dim=0)\n",
    "    test_dataset = TensorDataset(dd,ll)\n",
    "    '''\n",
    "    # Define the sizes for your train and test sets\n",
    "    #train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "    #test_size = len(dataset) - train_size  # 20% for testing\n",
    "    # Split the dataset into train and test\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    # Create DataLoaders\n",
    "    batch_size = 64  # Adjust as neededs\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    torch.save(train_dataset,f'models\\\\train_DWT_LaFraAnd_{i}.pt')\n",
    "    torch.save(test_dataset,f'models\\\\test_DWT_LaFraAnd_{i}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-fold cv LaFra fraGio   GioLA                      Ternary-class\n",
    "\n",
    "86.7           88.5            94.6                 85.2\n",
    "82.6           92.3            93.9                 89.2\n",
    "90.5           93.7            98.5                 96\n",
    "88.6           93.1            92.9                 92.5\n",
    "95.5           88.4            92.1                 89.2\n",
    "               Medie\n",
    "88.78           91.2           94.4s                90.4\n",
    "            media tot: 91.5\n",
    "\n",
    "            Train 100 epochs 0.001, 20 epochs 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with DWT                                            ternary                    4classes\n",
    "91.2            94.2            94.4                95.5    88                95.9            \n",
    "90.2            95.6            96.7                95.5    86                95.1\n",
    "92.7            93.9            98.5                96.2    87.8              99\n",
    "91.9            96.1            98.2                94.9                      72.4\n",
    "88.4            93.9            97.2                                          72.3    \n",
    "\n",
    "media = 90.9    94.74           97                  95.5                        87\n",
    "media delle medie = 94.21   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
