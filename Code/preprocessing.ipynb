{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from my_utils import butterworth_filter as butter_filt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, amplitude_data, phase_data, labels):\n",
    "        self.amplitude_data = amplitude_data\n",
    "        self.phase_data = phase_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amplitude_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        amp_window = torch.tensor(self.amplitude_data[idx], dtype=torch.float32)\n",
    "        phase_window = torch.tensor(self.phase_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return amp_window, phase_window, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_amplitude(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = butter_filt(df)\n",
    "    \n",
    "    # Normalize the entire series with MinMax scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def load_phase(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def create_windows(data, window_size=100, overlap=50, mean_reduction_ratio=4, start_ind = 15):\n",
    "    step = window_size - overlap\n",
    "    windows = []\n",
    "    for start in range(start_ind, len(data) - window_size + 1, step):\n",
    "        window = data.iloc[start:start + window_size].reset_index(drop=True)\n",
    "        reduced_window = window.groupby(window.index // mean_reduction_ratio).mean()\n",
    "        windows.append(reduced_window.values)\n",
    "    return windows\n",
    "\n",
    "def split_data(data, window_size=100, overlap=50, mean_reduction_ratio=4, train_ratio=0.8):\n",
    "    # Calculate split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    \n",
    "    # Create windows for training and testing\n",
    "    train_data = data.iloc[:split_index]\n",
    "    test_data = data.iloc[split_index:]\n",
    "    \n",
    "    # Generate windows\n",
    "    train_windows = create_windows(train_data, window_size, overlap, mean_reduction_ratio, start_ind=15)\n",
    "    test_windows = create_windows(test_data, window_size, overlap, mean_reduction_ratio, start_ind=0)\n",
    "    \n",
    "    return train_windows, test_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_csvs(amplitude_file_groups, phase_file_groups, window_size=100, overlap=50, mean_reduction_ratio=4, train_ratio=0.8):\n",
    "    all_train_amp_windows = []\n",
    "    all_test_amp_windows = []\n",
    "    all_train_phase_windows = []\n",
    "    all_test_phase_windows = []\n",
    "    all_labels_train = []\n",
    "    all_labels_test = []\n",
    "\n",
    "    class_counts_train = []\n",
    "    class_counts_test = []\n",
    "    \n",
    "    for class_index, (amp_files, phase_files) in enumerate(zip(amplitude_file_groups, phase_file_groups)):\n",
    "        class_train_amp_windows = []\n",
    "        class_test_amp_windows = []\n",
    "        class_train_phase_windows = []\n",
    "        class_test_phase_windows = []\n",
    "        \n",
    "        # Process each file for this class\n",
    "        for amp_file, phase_file in zip(amp_files, phase_files):\n",
    "            amp_file = \"..\\Data\\DataClean\\\\\" + amp_file\n",
    "            phase_file = \"..\\Data\\DataClean\\\\\" + phase_file\n",
    "            amp_data = load_and_preprocess_amplitude(amp_file)\n",
    "            phase_data = load_phase(phase_file)\n",
    "            \n",
    "            # Split data into training and testing\n",
    "            train_amp_windows, test_amp_windows = split_data(amp_data, window_size, overlap, mean_reduction_ratio, train_ratio)\n",
    "            train_phase_windows, test_phase_windows = split_data(phase_data, window_size, overlap, mean_reduction_ratio, train_ratio)\n",
    "            \n",
    "            # Aggregate windows for this class\n",
    "            class_train_amp_windows.extend(train_amp_windows)\n",
    "            class_test_amp_windows.extend(test_amp_windows)\n",
    "            class_train_phase_windows.extend(train_phase_windows)\n",
    "            class_test_phase_windows.extend(test_phase_windows)\n",
    "        \n",
    "        # Assign labels\n",
    "        num_train_windows = len(class_train_amp_windows)\n",
    "        num_test_windows = len(class_test_amp_windows)\n",
    "        \n",
    "        class_labels_train = [class_index] * num_train_windows\n",
    "        class_labels_test = [class_index] * num_test_windows\n",
    "        \n",
    "        all_train_amp_windows.extend(class_train_amp_windows)\n",
    "        all_test_amp_windows.extend(class_test_amp_windows)\n",
    "        all_train_phase_windows.extend(class_train_phase_windows)\n",
    "        all_test_phase_windows.extend(class_test_phase_windows)\n",
    "        all_labels_train.extend(class_labels_train)\n",
    "        all_labels_test.extend(class_labels_test)\n",
    "        \n",
    "        # Track counts\n",
    "        class_counts_train.append(num_train_windows)\n",
    "        class_counts_test.append(num_test_windows)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(all_train_amp_windows, all_train_phase_windows, all_labels_train)\n",
    "    test_dataset = TimeSeriesDataset(all_test_amp_windows, all_test_phase_windows, all_labels_test)\n",
    "    \n",
    "    # Print number of input points for each class\n",
    "    for class_index, (train_count, test_count) in enumerate(zip(class_counts_train, class_counts_test)):\n",
    "        print(f\"Class {class_index}: Train points = {train_count}, Test points = {test_count}\")\n",
    "    \n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Train points = 1444, Test points = 354\n",
      "Class 1: Train points = 1448, Test points = 356\n",
      "Class 2: Train points = 1462, Test points = 359\n",
      "Class 3: Train points = 1468, Test points = 360\n",
      "Class 4: Train points = 1553, Test points = 378\n",
      "Class 5: Train points = 1511, Test points = 369\n"
     ]
    }
   ],
   "source": [
    "# File paths for amplitude and phase data, grouped by class\n",
    "amplitude_file_groups = [\n",
    "    ['andrea1_amp.csv', 'andrea2_amp.csv', 'andrea3_amp.csv', 'andrea4_amp.csv', 'andrea5_amp.csv', 'andrea6_amp.csv'],\n",
    "    ['gio1_amp.csv', 'gio2_amp.csv', 'gio3_amp.csv', 'gio4_amp.csv', 'gio5_amp.csv', 'gio6_amp.csv'],\n",
    "    ['jacopo1_amp.csv', 'jacopo2_amp.csv', 'jacopo3_amp.csv', 'jacopo4_amp.csv', 'jacopo5_amp.csv', 'jacopo6_amp.csv'],\n",
    "    ['katy1_amp.csv', 'katy2_amp.csv', 'katy3_amp.csv', 'katy4_amp.csv', 'katy5_amp.csv', 'katy6_amp.csv'],\n",
    "    ['laura1_amp.csv', 'laura2_amp.csv', 'laura3_amp.csv', 'laura4_amp.csv', 'laura5_amp.csv', 'laura6_amp.csv', 'laura7_amp.csv', 'lauraDay2_1_amp.csv', 'lauraDay2_2_amp.csv', 'lauraDay2_3_amp.csv', 'lauraDay2_4_amp.csv'],\n",
    "    ['fra1_amp.csv', 'fra2_amp.csv', 'fra3_amp.csv', 'fra4_amp.csv', 'fra5_amp.csv', 'fra6_amp.csv', 'fra7_amp.csv', 'fra8_amp.csv']\n",
    "    # Add more groups for other classes\n",
    "]\n",
    "\n",
    "phase_file_groups = [\n",
    "    ['andrea1_phase.csv', 'andrea2_phase.csv', 'andrea3_phase.csv', 'andrea4_phase.csv', 'andrea5_phase.csv', 'andrea6_phase.csv'],\n",
    "    ['gio1_phase.csv', 'gio2_phase.csv', 'gio3_phase.csv', 'gio4_phase.csv', 'gio5_phase.csv', 'gio6_phase.csv'],\n",
    "    ['jacopo1_phase.csv', 'jacopo2_phase.csv', 'jacopo3_phase.csv', 'jacopo4_phase.csv', 'jacopo5_phase.csv', 'jacopo6_phase.csv'],\n",
    "    ['katy1_phase.csv', 'katy2_phase.csv', 'katy3_phase.csv', 'katy4_phase.csv', 'katy5_phase.csv', 'katy6_phase.csv'],\n",
    "    ['laura1_phase.csv', 'laura2_phase.csv', 'laura3_phase.csv', 'laura4_phase.csv', 'laura5_phase.csv', 'laura6_phase.csv', 'laura7_phase.csv', 'lauraDay2_1_phase.csv', 'lauraDay2_2_phase.csv', 'lauraDay2_3_phase.csv', 'lauraDay2_4_phase.csv'],\n",
    "    ['fra1_phase.csv', 'fra2_phase.csv', 'fra3_phase.csv', 'fra4_phase.csv', 'fra5_phase.csv', 'fra6_phase.csv', 'fra7_phase.csv', 'fra8_phase.csv']\n",
    "    # Add more groups for other classes if needed\n",
    "]\n",
    "\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset, test_dataset = create_dataset_from_csvs(amplitude_file_groups, phase_file_groups, window_size=100, overlap=50, mean_reduction_ratio=2, train_ratio=0.8)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
